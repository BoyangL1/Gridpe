# GridPE: Grid Cell-Inspired Positional Encoding for Vision and Point Cloud Transformers

This repository implements **GridPE**, a biologically inspired positional embedding method based on periodic spatial codes from grid cells. GridPE approximates shift-invariant attention kernels using structured Fourier embeddings and is applicable to both 2D vision and 3D point cloud transformers.

## ðŸ“‚ Project Structure

Gridpe/
â”œâ”€â”€ VIT/               # Vision Transformer (ViT) experiments and evaluation
â”œâ”€â”€ PCT/               # Point Cloud Transformer models and evaluation
â”œâ”€â”€ Simulation/        # Grid cell simulation and frequency analysis
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt

### ðŸ”¹ `VIT/`
- Contains ViT model variants with different positional embeddings: Learnable, RoPE, and GridPE.
- Includes attention distance/entropy analysis notebooks and visualization scripts.

### ðŸ”¹ `PCT/`
- Implements GridPE-based Point Cloud Transformer variants.
- Includes training/testing scripts, model definitions, and evaluation tools.

### ðŸ”¹ `Simulation/`
- Jupyter notebooks for simulating periodic grid cell responses and studying multi-scale frequency effects.

### ðŸ”¹ `deit/` *(inside `VIT/`)*
- Pretrained DeiT backbones and positional embedding modifications.

### ðŸ”¹ `GridAttn.py`
- Core implementation of the GridPE attention mechanism.

## ðŸ“Š Evaluation

Evaluation scripts and figures are available in:
- `VIT/2d_attn`: Visualization of attention statistics across image sizes.
- `PCT/3d_attn`: Similar analyses on point cloud models.

## ðŸ“¦ Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```


ðŸ“¬ Contact

For questions or collaborations, feel free to reach out via [liboyang0209@gmail.com].